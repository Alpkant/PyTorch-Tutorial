{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a real scenario on a toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using a GPU (if you have one definitely use it)  \n",
    "You should know one thing about the GPU training.  \n",
    "Your data and your model should be located at the same place to start training.  \n",
    "You can always move your data or model by using .to(device)\n",
    "or give function to as argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set our hyper parameters which are one of the most important thing for our training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 28 * 28    # 784\n",
    "num_classes = 10\n",
    "hidden_size = 500\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset (images and labels)\n",
    "train_dataset = torchvision.datasets.MNIST(root='data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader (input pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know how to use trained torch models but what if we want to create a custom model.  \n",
    "P.S. network architecture creation is very hard if your knowledge is limited. So customizing known architecture is good place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you create so much layer and giving unique name for each one is hard so we use nn.Sequential for those cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(input_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the model and move it to the device (GPU or CPU) default is CPU\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define our loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now important thing is we should write a training loop.  \n",
    "There could be so many different training loop as you want but every training loop has same simple logic\n",
    "\n",
    "1. Get data and move it to appropriate location\n",
    "2. Make a forward pass\n",
    "3. Calculate loss\n",
    "4. Zero the gradients\n",
    "5. Calculate gradients\n",
    "6. Optimize your network with one step gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [200/600], Loss: 0.2628\n",
      "Epoch [1/5], Step [400/600], Loss: 0.1714\n",
      "Epoch [1/5], Step [600/600], Loss: 0.2213\n",
      "Epoch [2/5], Step [200/600], Loss: 0.0605\n",
      "Epoch [2/5], Step [400/600], Loss: 0.1332\n",
      "Epoch [2/5], Step [600/600], Loss: 0.1167\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0753\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0496\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0960\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0897\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0549\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0570\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0134\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0663\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0744\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "# At each epoch we iterate over all the dataset\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move labels and images to the configured device\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        outputs = model(images)\n",
    "        # Calculate your loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        # If you don't accumulate the gradients then you should\n",
    "        # zero grads before calculate them\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Make a step with your optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # To monitor the training process we print or log some useful values \n",
    "        if (i+1) % 200 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97.84 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "# no_grad context saves you about forgetting gradient freezing\n",
    "# Within no_grad context nothing will compute or keep gradients\n",
    "# If you don't use no_grad your memory and  time usage will be higher\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        # If you don't use no_grad context you can use\n",
    "        # model.eval() function\n",
    "        # When you use it your model enters to evaluation mode (no grad calculation) \n",
    "        # Be careful some layers (BatchNorm) behaves different in training and evaluation mode \n",
    "        # You know we calculate local gradients when we do forward pass \n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Get predictions and calculate your accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome in a less than a minute you solved digit recognition problem with high accuracy.  \n",
    "Let's save our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
